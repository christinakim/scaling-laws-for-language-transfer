{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'zstandard'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d14c284721b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mzstandard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjsonlines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'zstandard'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zstandard\n",
    "import json\n",
    "import jsonlines\n",
    "import io\n",
    "import datetime\n",
    "\n",
    "def json_serial(obj):\n",
    "    \"\"\"JSON serializer for objects not serializable by default json code\"\"\"\n",
    "\n",
    "    if isinstance(obj, (datetime.datetime,)):\n",
    "        return obj.isoformat()\n",
    "    raise TypeError (\"Type %s not serializable\" % type(obj))\n",
    "\n",
    "# Modified version of lm_dataformat Archive for single file.\n",
    "class Archive:\n",
    "    def __init__(self, file_path, compression_level=3):\n",
    "        self.file_path = file_path\n",
    "        dir_name = os.path.dirname(file_path)\n",
    "        if dir_name:\n",
    "            os.makedirs(dir_name, exist_ok=True)    \n",
    "        self.fh = open(self.file_path, 'wb')\n",
    "        self.cctx = zstandard.ZstdCompressor(level=compression_level)\n",
    "        self.compressor = self.cctx.stream_writer(self.fh)        \n",
    "    \n",
    "    def add_data(self, data, meta={}):\n",
    "        self.compressor.write(json.dumps({'text': data, 'meta': meta}, default=json_serial).encode('UTF-8') + b'\\n')\n",
    "    \n",
    "    def commit(self):\n",
    "        self.compressor.flush(zstandard.FLUSH_FRAME)        \n",
    "        self.fh.flush()\n",
    "        self.fh.close()\n",
    "\n",
    "# Modified version of lm_dataformat Reader with self.fh set, allowing peeking for tqdm.\n",
    "class Reader:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def read_jsonl(self, file, get_meta=False, autojoin_paragraphs=True, para_joiner='\\n\\n'):\n",
    "        with open(file, 'rb') as fh:\n",
    "            self.fh = fh\n",
    "            cctx = zstandard.ZstdDecompressor()\n",
    "            reader = io.BufferedReader(cctx.stream_reader(fh))\n",
    "            rdr = jsonlines.Reader(reader)\n",
    "            for ob in rdr:\n",
    "                # naive jsonl where each object is just the string itself, with no meta. For legacy compatibility.\n",
    "                if isinstance(ob, str):\n",
    "                    assert not get_meta\n",
    "                    yield ob\n",
    "                    continue\n",
    "\n",
    "                text = ob['text']\n",
    "\n",
    "                if autojoin_paragraphs and isinstance(text, list):\n",
    "                    text = para_joiner.join(text)\n",
    "\n",
    "                if get_meta:\n",
    "                    yield text, (ob['meta'] if 'meta' in ob else {})\n",
    "                else:\n",
    "                    yield text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import math\n",
    "\n",
    "import tqdm\n",
    "\n",
    "document_count = 0\n",
    "total_text_size = 0\n",
    "dataset_directory = \"/datadrive/openwebtext2\"\n",
    "files = glob.glob(os.path.join(dataset_directory, \"*jsonl.zst\"))\n",
    "\n",
    "archives = [Archive(\"{}/shards/shard_{}\".format(dataset_directory,i)) for i in range(100)]\n",
    "archives_dict = {i:a for i, a in enumerate(archives)}\n",
    "\n",
    "for file_path in tqdm.tqdm(files, dynamic_ncols=True):\n",
    "    reader = Reader()\n",
    "    for document, metadata in reader.read_jsonl(file_path, get_meta=True):\n",
    "        document_count += 1\n",
    "        archive_index = document_count % 100\n",
    "        archives_dict[archive_index].add_data(document)\n",
    "        total_text_size += len(document)\n",
    "\n",
    "for a in archives:\n",
    "    a.commit()\n",
    "\n",
    "billion = math.pow(10, 9)\n",
    "print(f\"Total Document Count: {document_count:,}\")\n",
    "print(f\"Total Uncompressed Text Size: {(total_text_size / billion):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.dataset import IterableDataset\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "from utils.vocabulary import Reader\n",
    "from utils.vocabulary import Vocab\n",
    "\n",
    "\n",
    "class WebTextDocumentIterator:\n",
    "    def __init__(self, dataset_paths):\n",
    "        self.dataset_paths = dataset_paths\n",
    "\n",
    "    def get_document(self, reader, path):\n",
    "        return reader.read_jsonl(path)\n",
    "\n",
    "    def __iter__(self):\n",
    "        reader = Reader()\n",
    "        for path in self.dataset_paths:\n",
    "            yield from self.get_document(reader, path)\n",
    "\n",
    "\n",
    "class FileIterator:\n",
    "    def __init__(self, dataset_paths):\n",
    "        self.dataset_paths = dataset_paths\n",
    "\n",
    "    def get_file(self, path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            yield from f.readlines()\n",
    "\n",
    "    def __iter__(self):\n",
    "        for path in self.dataset_paths:\n",
    "            yield from self.get_file(path)\n",
    "\n",
    "\n",
    "class TokenizerIterator:\n",
    "    def __init__(self, seq_len, tokenizer, dataset_paths):\n",
    "        self.seq_len = seq_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.document_iter = WebTextDocumentIterator(dataset_paths)\n",
    "\n",
    "    def tokenize_doc(self, x):\n",
    "        tokenized = self.tokenizer(text=x, truncation=True).input_ids\n",
    "        tokenized.append(self.tokenizer.eos_token_id)\n",
    "\n",
    "        tokenized.insert(0, self.tokenizer.eos_token_id)\n",
    "        if len(tokenized) >= self.seq_len:\n",
    "            for i in range(len(tokenized) - self.seq_len):\n",
    "                yield tokenized[i : i + self.seq_len], tokenized[i + 1 : i + 1 + self.seq_len], len(\n",
    "                    tokenized[i : i + self.seq_len]\n",
    "                )\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def __iter__(self):\n",
    "        for x in self.document_iter:\n",
    "            yield from self.tokenize_doc(x)\n",
    "\n",
    "\n",
    "class BatchIterator:\n",
    "    def __init__(self, seq_len, batch_size, drop_last, tokenizer, dataset_paths):\n",
    "\n",
    "        stream_split = len(dataset_paths) // batch_size\n",
    "        streams = [dataset_paths[(i*stream_split):((i+1)*stream_split)] for i in range(0, batch_size)]\n",
    "        self.tokenizers = [TokenizerIterator(\n",
    "            seq_len=seq_len, tokenizer=tokenizer, dataset_paths=stream\n",
    "        ) for stream in streams]\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        data_list, label_list, seq_len_list = [], [], []\n",
    "        for _data, _label, _seq in batch:\n",
    "            data_list.append(_data)\n",
    "            label_list.append(_label)\n",
    "            seq_len_list.append(_seq)\n",
    "        return (\n",
    "            torch.LongTensor(data_list).permute(1,0),\n",
    "            torch.LongTensor(label_list).permute(1,0),\n",
    "            torch.LongTensor(seq_len_list),\n",
    "        )\n",
    "    \n",
    "    def get_stream(self, data_list):\n",
    "        return chain.from_iterable(map(self.process_data, cycle(data_list)))\n",
    "    \n",
    "    def get_streams(self):\n",
    "        return zip(*[self.get_stream(self.shuffled_data_list) for _ in range(self.batch_size)])\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self.get_streams()\n",
    "\n",
    "    def __iter__(self):\n",
    "        batch = []\n",
    "        try:\n",
    "            while True:\n",
    "                for tokenizer in self.tokenizers:\n",
    "                    a = next(iter(tokenizer))\n",
    "                    batch.append(a)\n",
    "                if len(batch) == self.batch_size:\n",
    "                    yield self.collate_fn(batch)\n",
    "                    batch = []\n",
    "        except StopIteration:\n",
    "            return\n",
    "\n",
    "\n",
    "class WebTextIter(IterableDataset):\n",
    "    def __init__(self, batch_size, drop_last, dataset_paths, seq_len, tokenizer=None):\n",
    "        if tokenizer is None:\n",
    "            tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "        self.seq_len = seq_len\n",
    "        self.dataset_paths = dataset_paths\n",
    "        self.batch_iter = BatchIterator(\n",
    "            seq_len=seq_len,\n",
    "            batch_size=batch_size,\n",
    "            drop_last=drop_last,\n",
    "            tokenizer=tokenizer,\n",
    "            dataset_paths=dataset_paths,\n",
    "        )\n",
    "\n",
    "    def __iter__(self):\n",
    "        for x in self.batch_iter:\n",
    "            yield x\n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_directory = \"/datadrive/openwebtext2/shards/\"\n",
    "files = glob.glob(os.path.join(dataset_directory, \"*\"))\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.dataset import IterableDataset\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "from utils.vocabulary import Reader\n",
    "from utils.vocabulary import Vocab\n",
    "import random\n",
    "from itertools import chain, cycle, islice\n",
    "import torch.utils.data as data\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class WebTextDocumentIterator:\n",
    "    def __init__(self, dataset_paths):\n",
    "        self.dataset_paths = dataset_paths\n",
    "\n",
    "    def get_document(self, reader, path):\n",
    "        return reader.read_jsonl(path)\n",
    "\n",
    "    def __iter__(self):\n",
    "        reader = Reader()\n",
    "        for path in self.dataset_paths:\n",
    "            yield from self.get_document(reader, path)\n",
    "\n",
    "\n",
    "class FileIterator:\n",
    "    def __init__(self, dataset_paths):\n",
    "        self.dataset_paths = dataset_paths\n",
    "\n",
    "    def get_file(self, path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            yield from f.readlines()\n",
    "\n",
    "    def __iter__(self):\n",
    "        for path in self.dataset_paths:\n",
    "            yield from self.get_file(path)\n",
    "\n",
    "\n",
    "class TokenizerIterator:\n",
    "    def __init__(self, seq_len, tokenizer, dataset_paths):\n",
    "        self.seq_len = seq_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.document_iter = WebTextDocumentIterator(dataset_paths)\n",
    "\n",
    "    def tokenize_doc(self, x):\n",
    "        tokenized = self.tokenizer(text=x, truncation=True).input_ids\n",
    "        tokenized.append(self.tokenizer.eos_token_id)\n",
    "\n",
    "        tokenized.insert(0, self.tokenizer.eos_token_id)\n",
    "        if len(tokenized) >= self.seq_len:\n",
    "            for i in range(len(tokenized) - self.seq_len):\n",
    "                yield tokenized[i : i + self.seq_len], tokenized[i + 1 : i + 1 + self.seq_len], len(\n",
    "                    tokenized[i : i + self.seq_len]\n",
    "                )\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def __iter__(self):\n",
    "        for x in self.document_iter:\n",
    "            yield from self.tokenize_doc(x)\n",
    "\n",
    "\n",
    "class BatchIterator:\n",
    "    def __init__(self, seq_len, batch_size, drop_last, tokenizer, dataset_paths):\n",
    "\n",
    "        self.dataset_paths = dataset_paths\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        data_list, label_list, seq_len_list = [], [], []\n",
    "        for _data, _label, _seq in batch:\n",
    "            data_list.append(_data)\n",
    "            label_list.append(_label)\n",
    "            seq_len_list.append(_seq)\n",
    "        return (\n",
    "            torch.LongTensor(data_list).permute(1,0),\n",
    "            torch.LongTensor(label_list).permute(1,0),\n",
    "            torch.LongTensor(seq_len_list),\n",
    "        )\n",
    "    def process_data(self, dataset):\n",
    "        self.tokenizer_iter = TokenizerIterator(self.seq_len, tokenizer, [dataset])\n",
    "        for x in self.tokenizer_iter:\n",
    "            yield x\n",
    "            \n",
    "    def shuffled_data_list(self, i):\n",
    "        split = len(self.dataset_paths) // self.batch_size\n",
    "        dataset_paths = self.dataset_paths[(i*split):((i+1)*split)]\n",
    "        return random.sample(dataset_paths, len(dataset_paths))\n",
    "        \n",
    "    \n",
    "    def get_stream(self, data_list):\n",
    "        return chain.from_iterable(map(self.process_data, cycle(data_list)))\n",
    "    \n",
    "    def get_streams(self):\n",
    "        return zip(*[self.get_stream(self.shuffled_data_list(i)) for i in range(self.batch_size)])\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self.get_streams()\n",
    "\n",
    "\n",
    "\n",
    "class WebTextIter(IterableDataset):\n",
    "    def __init__(self, batch_size, drop_last, dataset_paths, seq_len, tokenizer=None):\n",
    "        if tokenizer is None:\n",
    "            tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "        self.seq_len = seq_len\n",
    "        self.dataset_paths = dataset_paths\n",
    "        self.batch_iter = BatchIterator(\n",
    "            seq_len=seq_len,\n",
    "            batch_size=batch_size,\n",
    "            drop_last=drop_last,\n",
    "            tokenizer=tokenizer,\n",
    "            dataset_paths=dataset_paths,\n",
    "        )\n",
    "\n",
    "    def __iter__(self):\n",
    "        for x in self.batch_iter:\n",
    "            yield self.collate_fn(x)\n",
    "\n",
    "            \n",
    "    def collate_fn(self, batch):\n",
    "        data_list, label_list, seq_len_list = [], [], []\n",
    "        for _data, _label, _seq in batch:\n",
    "            data_list.append(_data)\n",
    "            label_list.append(_label)\n",
    "            seq_len_list.append(_seq)\n",
    "        return (\n",
    "            torch.LongTensor(data_list).permute(1,0),\n",
    "            torch.LongTensor(label_list).permute(1,0),\n",
    "            torch.LongTensor(seq_len_list),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "wt = WebTextIter(dataset_paths=files[:20], batch_size=10, drop_last=True, seq_len=5)\n",
    "dl = DataLoader(wt, batch_size=None, sampler=None)\n",
    "count = 0\n",
    "seen = []\n",
    "for i, n in enumerate(dl):\n",
    "    print(n)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(4)\n",
    "random.shuffle(files)\n",
    "total_size = 0\n",
    "for path in files:\n",
    "    total_size += os.path.getsize(path)\n",
    "total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0\n",
    "test_files = []\n",
    " \n",
    "for path in files:\n",
    "    if test_size >= test:\n",
    "        break\n",
    "    file_size = os.path.getsize(path)\n",
    "    print(file_size)\n",
    "    if (test_size + file_size) >= test:\n",
    "        print(\"no\")\n",
    "        continue\n",
    "    else:\n",
    "        test_size += file_size\n",
    "        test_files.append(path)\n",
    "test_files, test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = 0\n",
    "val_files = []\n",
    " \n",
    "for path in files:\n",
    "    if path in test_files:\n",
    "        continue\n",
    "    if val_size >= val:\n",
    "        break\n",
    "    file_size = os.path.getsize(path)\n",
    "    if val_size + file_size >= val:\n",
    "        continue\n",
    "    else:\n",
    "        val_size += file_size\n",
    "    val_files.append(path)\n",
    "val_files, val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "for f in files:\n",
    "    fname = os.path.basename(f)\n",
    "\n",
    "    if f in val_files or f in test_files:\n",
    "        print(fname)\n",
    "        continue\n",
    "    shutil.move(\"/datadrive/openwebtext2/train/{}\".format(fname), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
