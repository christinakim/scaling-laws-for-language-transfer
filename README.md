# Scaling Laws for Language Transfer Learning

Building upon work from Scaling Laws for Transfer (Hernandez et. al. 2021), my experiments focused on exploring the relationships between fine-tuning on non-English languages. My experiments try to answer the question: How much does pre-training on English help when transferring across different languages as we vary the dataset size and model size?

Experiment results [here](https://christina.kim/2021/04/11/scaling-laws-for-language-transfer-learning/)
